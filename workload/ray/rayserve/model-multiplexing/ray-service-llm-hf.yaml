# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: ServiceAccount
metadata:
  name: ray-serve
---
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: model-multiplexing
spec:
  # Specifying accelerator_type: L4, creates a requirement for accelerator_type:L4, which
  # will create new replicas for existing workerGroup resources. the ray serve autoscaler works
  # Since accelerator_type:L4 is not a characteristic of the workerGroup, the autoscaling
  # of the cluster is not triggered
  # Specifying deployment_config.ray_actor_options.num_gpus causes each process to require a GPU
  serveConfigV2: |
    applications:
    - args:
        llm_configs:
          - model_loading_config:
              model_id: meta-llama/Llama-3.2-1B-Instruct
            engine_kwargs:
              tensor_parallel_size: 1
            deployment_config:
              max_ongoing_requests: 5
              autoscaling_config:
                target_ongoing_requests: 1
                min_replicas: 1
                max_replicas: 4
          # - model_loading_config:
          #     model_id: google/gemma-3-4b-it
          #   engine_kwargs:
          #     tensor_parallel_size: 1
          #     max_model_len: 4096
          #   deployment_config:
          #     autoscaling_config:
          #       target_ongoing_requests: 1
          #       min_replicas: 1
          #       max_replicas: 4
          # - model_loading_config:
          #     model_id: google/gemma-2-9b-it
          #   engine_kwargs:
          #     tensor_parallel_size: 2
          #   deployment_config:
          #     autoscaling_config:
          #       min_replicas: 1
          #       max_replicas: 2
          # - model_loading_config:
          #     model_id: meta-llama/Llama-3.1-8B-Instruct
          #   engine_kwargs:
          #     tensor_parallel_size: 2
          #   deployment_config:
          #     autoscaling_config:
          #       min_replicas: 1
          #       max_replicas: 2
      import_path: ray.serve.llm:build_openai_app
      name: llm_app
      route_prefix: "/"
      runtime_env:
        # worker dependency for httpx, at least with 2.45-2.46
        pip: ["httpx==0.28.1"]
  rayClusterConfig:
    enableInTreeAutoscaling: true
    autoscalerOptions:
      upscalingMode: Default
      idleTimeoutSeconds: 60
      imagePullPolicy: IfNotPresent
      # Optionally specify the Autoscaler container's securityContext.
      securityContext: {}
      env: []
      envFrom: []
      resources:
        limits:
          cpu: "500m"
          memory: "512Mi"
        requests:
          cpu: "500m"
          memory: "512Mi"
    headGroupSpec:
      rayStartParams:
        dashboard-host: "0.0.0.0"
      template:
        metadata:
          labels:
            ai.gke.io: rayserve
        spec:
          restartPolicy: Never
          containers:
            - name: ray-head
              #image: rayproject/ray-ml:2.46.0.0e19ea-py311-cpu
              image: rayproject/ray-llm:2.46.0-py311-cu124
              resources:
                limits:
                  cpu: "2"
                  memory: "8Gi"
                  ephemeral-storage: "10Gi"
                requests:
                  cpu: "2"
                  memory: "8Gi"
                  ephemeral-storage: "10Gi"
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 8000
                  name: serve
              env:
                - name: RAY_enable_autoscaler_v2
                  value: "1"
    workerGroupSpecs:
      - replicas: 1
        # Setting this to 0, when autoscaling is turned on, will scale
        # up then back to 0, until the serve replicas are ready to be spun up
        minReplicas: 1
        maxReplicas: 4
        groupName: gpu-group
        rayStartParams: {}
        template:
          metadata:
            labels:
              ai.gke.io: rayserve
              app: rayserve
          spec:
            restartPolicy: Never
            containers:
              - name: llm
                image: rayproject/ray-llm:2.46.0-py311-cu124
                env:
                  - name: HUGGING_FACE_HUB_TOKEN
                    valueFrom:
                      secretKeyRef:
                        name: hugging-face-token-secret
                        key: HUGGING_FACE_TOKEN
                resources:
                  limits:
                    cpu: "20"
                    memory: "40Gi"
                    nvidia.com/gpu: "2"
                  requests:
                    cpu: "20"
                    memory: "40Gi"
                    nvidia.com/gpu: "2"
            serviceAccountName: ray-serve
            nodeSelector:
              cloud.google.com/gke-accelerator: nvidia-l4
